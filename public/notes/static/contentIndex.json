{"Blog/tools":{"slug":"Blog/tools","filePath":"Blog/tools.md","title":"Tools That I Can't Live Without","links":[],"tags":[],"content":"Iâ€™ve always loved reading about how other people work â€” what tools they rely on to stay productive, focused, and creative. Itâ€™s rarely about the latest or flashiest gear; more often, itâ€™s the things that have quietly stood the test of time. The small-but-mighty tools and habits that keep the wheels turning day after day â€” thatâ€™s the stuff I find most interesting.\nAs someone who works at the intersection of data engineering, data science, and human rights â€” while also being a dad of two â€” Iâ€™ve come to rely on a handful of tools that just work. These arenâ€™t necessarily the most popular or expensive options, but theyâ€™ve earned their place by helping me stay efficient, focused, and (mostly) sane. Here are the essentials I canâ€™t live without â€” and a few honorable mentions that deserve a shout-out.\n\nğŸ§  Tools I Canâ€™t Live Without\nğŸ–¥ï¸ My Machine (MacBook Pro)\nIâ€™ve been a Linux user for many years and honestly never wouldâ€™ve bought a Mac unless my employer paid for it. But after years of constant tinkering, fixing, and configuring things under Linux, switching to a Mac â€” where âœ¨ everything just works âœ¨ â€” has been worth every second. It lets me focus on what I actually want to do: write code, build things, and not debug my environment.\nâŒ¨ï¸ My Keyboard (Kinesis Advantage2)\nThis thing looks like it belongs on a spaceship â€” and honestly, it kind of does. The Kinesis Advantage2 is an ergonomic, split-layout keyboard that took me a couple of weeks to get used to, but now I canâ€™t imagine working without it. Itâ€™s helped reduce wrist strain during long coding sessions, and the thumb clusters are a game-changer for common shortcuts. Expensive? Yes. Overkill? Maybe. But for me, itâ€™s an investment in longevity and focus â€” and one of the best Iâ€™ve made.\nğŸ§™â€â™‚ï¸ My Editor (Neovim)\nIâ€™ve tried a lot of editors, but Neovim is the one I keep coming back to. Itâ€™s fast, minimal, and lets me mold my environment exactly the way I want. Sure, the initial setup feels like a side project of its own, but once it clicks, itâ€™s incredibly productive. Whether Iâ€™m building data pipelines, debugging something messy, or just writing Markdown â€” being inside Neovim feels like home. Itâ€™s not for everyone, but for me, the control, speed, and focus it gives is unmatched.\nğŸ My Language (Python)\nIt might not be the fastest or the trendiest, but Python is the tool I reach for again and again. Itâ€™s the duct tape of data â€” flexible, readable, and supported everywhere. Whether Iâ€™m cleaning messy CSVs, writing ETL jobs, building machine learning models, or scripting a quick fix, Python gets it done without fuss. The ecosystem is massive, the libraries are mature, and the balance between readability and power fits perfectly with how I think. Itâ€™s not just a language for me â€” itâ€™s the foundation of my entire workflow.\nâœ¨ Honorable Mentions\nThese are tools I donâ€™t absolutely need every single day â€” but when I do use them, they make life noticeably better.\nğŸ“Š dbt\nIt took a while for me to â€œgetâ€ dbt, but once I did, it became a staple for transforming data cleanly and transparently. Itâ€™s like version control for SQL, and when paired with good documentation, it makes working with data models a joy â€” or at least less of a pain.\nğŸ“‹ Notion\nI donâ€™t manage my life in it, but itâ€™s where I draft blog posts, capture ideas, and plan projects. Itâ€™s the digital equivalent of a whiteboard â€” clean, structured, and there when I need it.\nğŸ§ª Jupyter Notebooks\nSometimes you just want to poke at data, test a theory, or sketch out a model. Notebooks are great for that. I donâ€™t build anything long-term in them, but for exploratory work, theyâ€™re still hard to beat.\nğŸ“ Texas Instruments TI-82 Stats\nThis oneâ€™s a bit of a wildcard, but it deserves a mention. The TI-82 Stats calculator was my first real exposure to the idea that math could be interactive â€” that I could test things, explore, and not just follow steps. I even started coding on it back in high school, making little games and cheats to get through math classes. Later, it helped me survive those chaotic calculus sessions in university. It may not be in my daily toolkit anymore, but in a way, itâ€™s where all of this started.\n\nThatâ€™s my current lineup â€” a mix of tools that help me stay productive, stay sane, and occasionally stay nostalgic. Iâ€™m always curious about what others rely on, so if youâ€™ve got a must-have tool, a weird keyboard, or a setup hack you swear by, Iâ€™d love to hear about it. Drop a comment or hit reply â€” Iâ€™m here for the gear talk.\nThanks for reading,\nâ€“ N"},"Statistics/Nonparametric-Hypothesis-testing/chi-squared-goodness-of-fit-testing":{"slug":"Statistics/Nonparametric-Hypothesis-testing/chi-squared-goodness-of-fit-testing","filePath":"Statistics/Nonparametric Hypothesis testing/chi-squared-goodness-of-fit-testing.md","title":"Chi Squared Goodness of fit testing","links":[],"tags":[],"content":"What is the purpose of the test\nSuppose you have a dataset, and you want to evaluate how well it aligns with a particular discrete probability distribution. The Chi-Squared test helps you assess whether the observed frequencies in your dataset deviate significantly from what youâ€™d expect under the hypothesized distribution.\nAssumptions\n\nThe data is binned into discrete categories (e.g. 5 bins).\nThe sample size in each bin is reasonably large (typically â‰¥ 5 expected counts per bin).\nObservations are independent.\n\nThe Test\nThe test statistic is computed as:\n\nT_n = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\n\nWhere:\n\n( O_i ): Observed frequency in bin ( i )\n( E_i ): Expected frequency in bin ( i ) under the null hypothesis\n( k ): Number of bins\n\nLarge values of ( T_n ) suggest a poor fit between the data and the expected distribution.\nExample\nFirst lets generate some poisson data with a \\lambda of 3.\nbin_edges = np.arange(-0.5, 10.5, 1)\nbin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n \nnp.random.seed()\npoisson_data = poisson.rvs(mu=3, size=1000)\n \nobserved, _ = np.histogram(poisson_data, bins=bin_edges)\n\nWhich we can see is definitely poisson in its distribution.\nexpected_poisson = poisson.pmf(np.arange(len(observed)), mu=3) * observed.sum()\nexpected_gaussian = (\n    norm.pdf(bin_centers, loc=3, scale=1)\n    * (bin_edges[1] - bin_edges[0])\n    * observed.sum()\n)\n \nexpected_poisson *= observed.sum() / expected_poisson.sum()\nexpected_gaussian *= observed.sum() / expected_gaussian.sum()\n \nstat_poisson, p_poisson = chisquare(f_obs=observed, f_exp=expected_poisson)\nstat_gaussian, p_gaussian = chisquare(f_obs=observed, f_exp=expected_gaussian)\nWhich in turn gives us:\n\\begin{aligned}\nT_{poiss}=1.8236827300439549\\\\\np_{poiss}=0.9939587753318262\\\\\n\\\\\nT_{N}=698370.901071352\\\\\np_{N}=0.0\n\\end{aligned}\nGiven the high p-value under the Poisson model and the extremely low p-value under the Gaussian model, the evidence strongly favours the Poisson distribution for this data.\nDegrees of freedom\ndf=\\text{number of bins}-1-\\text{number of estimated parameters}\nFor example, if you have:\n\n5 bins\nand estimate 2 parameters (like the mean and variance of a Gaussian)\n\nThen:\ndf=5-1-2=2\nGood to know\n\nThe Chi-Squared test is asymptotic, so the approximation becomes better as n \\to \\infty.\nFor small sample sizes, you should use exact p-values from tables.\n\nYou can find \\chi^2 tables here"},"Statistics/Nonparametric-Hypothesis-testing/kolmogorov-lilliefors":{"slug":"Statistics/Nonparametric-Hypothesis-testing/kolmogorov-lilliefors","filePath":"Statistics/Nonparametric Hypothesis testing/kolmogorov-lilliefors.md","title":"The Kolmogorov-Lilliefors Test","links":["Statistics/Nonparametric-Hypothesis-testing/kolmogorov-smirnov"],"tags":[],"content":"The Lilliefors test is a modification of the Kolmogorovâ€“Smirnov (KS) test that accounts for the fact that we often donâ€™t know the population mean and variance in advance â€” and instead estimate them from the sample.\nThis violates the assumptions of the regular KS test, which requires those parameters to be fixed. The Lilliefors test corrects for that by adjusting the distribution of the test statistic.\nThe null hypothesis assumes normality, but allows the parameters to be estimated from the sample. This makes the Lilliefors test more realistic than the classical KS test for normality testing.\nWhen to Use It\n\nYouâ€™re testing for normality.\nYou estimated the mean and standard deviation from your sample.\nYour sample is continuous (not binned or discrete).\n\nThe Test\nLet ( D ) be the maximum distance between the empirical CDF of your sample and the theoretical CDF of a normal distribution fit to your data (i.e., using the sample mean and standard deviation). The Lilliefors test uses the same test statistic as KS:\nD_n = \\sup_x \\left| F_n(x) - F(x; \\hat{\\mu}, \\hat{\\sigma}) \\right|\nBut it compares it to a different null distribution, accounting for the parameter estimation.\nExample (Python)\nimport numpy as np\n \nfrom statsmodels.stats.diagnostic import lilliefors\n \n# Generate a sample from a normal distribution\nnp.random.seed(42)\nnormal_data = np.random.normal(loc=0, scale=1, size=1000)\n \n# Run the Lilliefors test (KS test adjusted for estimated mean and std)\nstat_lillie, p_lillie = lilliefors(normal_data)\n \nThis gives us:\n\\begin{aligned}\nD_{Lilliefors} &amp;= 0.0214 \\\\\np &amp;= 0.4051\n\\end{aligned}\nBecause the p-value is relatively high, we fail to reject the null hypothesis â€” thereâ€™s no strong evidence that the data deviates from normality.\nSummary\n\nUse the Lilliefors test instead of KS when youâ€™re testing normality and have fit the parameters.\nThe output is a KS-like statistic and a p-value based on simulations or adjusted tables.\nItâ€™s a quick and useful test for normality when you canâ€™t assume known parameters.\n"},"Statistics/Nonparametric-Hypothesis-testing/kolmogorov-smirnov":{"slug":"Statistics/Nonparametric-Hypothesis-testing/kolmogorov-smirnov","filePath":"Statistics/Nonparametric Hypothesis testing/kolmogorov-smirnov.md","title":"The Kolmogorovâ€“Smirnov (KS) Test","links":["Statistics/Nonparametric-Hypothesis-testing/chi-squared-goodness-of-fit-testing"],"tags":[],"content":"The KS test is a non-parametric method used to determine whether a sample comes from a specific continuous distribution.\nUnlike the Chi-Squared test, which compares binned frequencies, the KS test compares the empirical cumulative distribution function (ECDF) of your sample with the theoretical cumulative distribution function (CDF) of the hypothesised distribution.\nPurpose\nTo measure the maximum vertical distance between the ECDF and the CDF of a specified distribution. Formally, the test statistic is:\nD_n = \\sup_x |F_n(x) - F(x)|\n\n( F_n(x) ): empirical CDF from the sample\n( F(x) ): theoretical CDF under the null hypothesis\n( D_n ): KS test statistic\n\nAssumptions\n\nThe hypothesised distribution is continuous\nThe sample is independent and identically distributed (i.i.d.)\nNo ties in the data (KS is sensitive to discrete data)\n\nExample: Testing Poisson Data\nTo demonstrate the KS test in its natural setting, letâ€™s generate data from a standard normal distribution ( \\mathcal{N}(0,1) ) and compare it against:\n\nThe correct distribution: ( \\mathcal{N}(0,1) )\nA slightly incorrect distribution: ( \\mathcal{N}(1,1) )\n\nfrom scipy.stats import norm, kstest\nimport numpy as np\n \nnp.random.seed(42)\nnormal_data = norm.rvs(loc=0, scale=1, size=1000)\n \n# Test against correct distribution\nks_stat_true, p_val_true = kstest(normal_data, &#039;norm&#039;, args=(0, 1))\n \n# Test against incorrect distribution\nks_stat_wrong, p_val_wrong = kstest(normal_data, &#039;norm&#039;, args=(1, 1))\nThis yields:\n\\begin{aligned}\nD_{\\mathcal{N}(0,1)} &amp;= 0.0173 \\\\\np_{\\mathcal{N}(0,1)} &amp;= 0.9197 \\\\\n\\\\\nD_{\\mathcal{N}(1,1)} &amp;= 0.3916 \\\\\np_{\\mathcal{N}(1,1)} &amp;= 1.11 \\times 10^{-138}\\\\\n\\\\\n\\end{aligned}\nInterpretation\n\nThe high p-value when testing against (\\mathcal{N}(0,1) ) suggests the sample is consistent with the hypothesised distribution.\nThe extremely low p-value when testing against ( \\mathcal{N}(1,1) ) strongly rejects the fit.\n"},"Stupid-data-science/ufo":{"slug":"Stupid-data-science/ufo","filePath":"Stupid data science/ufo.md","title":"Here Are the Most UFO-Friendly States in the U.S.","links":[],"tags":[],"content":"\nWhere in America are you most likely to spot something strange in the sky â€” and actually report it?\n\nEach year, thousands of Americans file reports of unidentified flying objects. But not all states are created equal when it comes to scanning the skies. Some places seem to be absolute UFO magnets â€” and itâ€™s not always where youâ€™d expect.\nSo we asked ourselves:\nWhat makes a state â€œUFO-friendlyâ€?\nWe landed on two ingredients:\n\nğŸ›¸ How often people report UFO sightings (scaled per million residents)\nğŸŒ¾ How rural the state is, which we used as a rough stand-in for things like open skies, less light pollution, and maybe â€” just maybe â€” a stronger belief in the unexplained.\n\nTogether, these make up our UFO Visibility Index â€” a totally unscientific but super fun score to rank every U.S. stateâ€™s UFO-spotting potential.\nLetâ€™s dig in.\n\nğŸ‘€ UFO Sightings per Million People\n\nThis map shows how many UFO sightings each state has reported per million residents â€” giving small but active states like Vermont a fair shot against behemoths like California.\nğŸ§  Top Performers\n\nVermont, Montana, and Washington top the list with the highest sightings-per-million.\nThe Pacific Northwest and Northeast seem especially active â€” is it all the forests? The coffee? Or just more sky-gazing?\n\nWhatever it is, these folks are paying attention.\n\nğŸŒ¾ Rural Percentage of Population\n\nWhy does rurality matter? Well, it could mean clearer skies, fewer city lights, and maybe even a greater willingness to believe or report something strange up there.\nThis map shows what percent of each stateâ€™s population lives in rural areas.\nğŸï¸ Leaders in Rural Life\n\nVermont, Maine, and West Virginia take the rural crown.\nThe most urbanized? California, Nevada, and New Jersey â€” all clocking in under 10% rural population.\n\nNot saying rural folks believe more in aliens, butâ€¦ they do have better stargazing conditions. ğŸ‘€\n\nğŸš€ The UFO Visibility Index\n\nNow comes the fun part.\nWe combined the two normalized metrics â€” sightings-per-million and rural population percentage â€” into a single UFO Visibility Index using Min-Max scaling.\nSo what does this index show?\n\nIt highlights states where people are reporting sightings and live in areas where sky visibility is likely better.\nHigher index = better â€œUFO spotting conditionsâ€ (in theory, of course ğŸ˜).\n\nğŸ¥‡ Top UFO-Friendly States:\n\nMaine, Alaska, and Montana dominate the rankings.\nThese states strike a curious balance: lots of rural land and people eager to report whatâ€™s flying through it.\n\n\nğŸ§ª Method Notes\nThis was all done in a Python notebook â€” no fancy modeling, just some data wrangling, normalization, and choropleth mapping for fun.\nIf youâ€™re curious, the correlation between rural population percentage and sightings-per-million sits at around 0.25. Not huge, but definitely not nothing.\nCheck out the full notebook here to explore the code and data sources.\n\nğŸ‘‹ Final Thoughts\nSo there you have it â€” if youâ€™re hoping to catch something weird in the sky, Vermont, Maine, Alaska, and Montana might be your best bets. Whether itâ€™s clearer skies, fewer city lights, or just a greater sense of curiosity, some states definitely seem more tuned in to whatâ€™s up there.\nWant to poke around the data yourself or see how this was built?\nCheck out the full notebook and code here.\nKeep your eyes on the skies ğŸ‘½\n\nğŸ“š Sources\n1. Kaggle (n.d.)\nUFO Sightings. Available at: www.kaggle.com/datasets/NUFORC/ufo-sightings (Accessed: 22 June 2025).\n2. United States Census Bureau (2024)\nState Population Totals and Components of Change: 2020â€“2024. Available at: www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html (Accessed: 22 June 2025).\n3. Federal Housing Finance Agency (FHFA) (2023)\nWho Lives in Rural America? Available at: www.fhfa.gov/blog/insights/who-lives-in-rural-america (Accessed: 22 June 2025).\n4. United Health Foundation (2025)\nExplore Rural Population in the United States. Americaâ€™s Health Rankings. Available at: www.americashealthrankings.org/explore/measures/rural_population (Accessed: 22 June 2025)."},"about":{"slug":"about","filePath":"about.md","title":"About This Site","links":[],"tags":[],"content":"This is a test note in Quartz."},"index":{"slug":"index","filePath":"index.md","title":"Welcome to My Notes","links":["about"],"tags":[],"content":"Welcome\nThis is a public digital garden built with Quartz.\nCurrently the site is under construction, please come back later\nCheck out about for more info."}}