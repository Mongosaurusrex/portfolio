{"Blog/tools":{"slug":"Blog/tools","filePath":"Blog/tools.md","title":"Tools That I Can't Live Without","links":[],"tags":[],"content":"I’ve always loved reading about how other people work — what tools they rely on to stay productive, focused, and creative. It’s rarely about the latest or flashiest gear; more often, it’s the things that have quietly stood the test of time. The small-but-mighty tools and habits that keep the wheels turning day after day — that’s the stuff I find most interesting.\nAs someone who works at the intersection of data engineering, data science, and human rights — while also being a dad of two — I’ve come to rely on a handful of tools that just work. These aren’t necessarily the most popular or expensive options, but they’ve earned their place by helping me stay efficient, focused, and (mostly) sane. Here are the essentials I can’t live without — and a few honorable mentions that deserve a shout-out.\n\n🧠 Tools I Can’t Live Without\n🖥️ My Machine (MacBook Pro)\nI’ve been a Linux user for many years and honestly never would’ve bought a Mac unless my employer paid for it. But after years of constant tinkering, fixing, and configuring things under Linux, switching to a Mac — where ✨ everything just works ✨ — has been worth every second. It lets me focus on what I actually want to do: write code, build things, and not debug my environment.\n⌨️ My Keyboard (Kinesis Advantage2)\nThis thing looks like it belongs on a spaceship — and honestly, it kind of does. The Kinesis Advantage2 is an ergonomic, split-layout keyboard that took me a couple of weeks to get used to, but now I can’t imagine working without it. It’s helped reduce wrist strain during long coding sessions, and the thumb clusters are a game-changer for common shortcuts. Expensive? Yes. Overkill? Maybe. But for me, it’s an investment in longevity and focus — and one of the best I’ve made.\n🧙‍♂️ My Editor (Neovim)\nI’ve tried a lot of editors, but Neovim is the one I keep coming back to. It’s fast, minimal, and lets me mold my environment exactly the way I want. Sure, the initial setup feels like a side project of its own, but once it clicks, it’s incredibly productive. Whether I’m building data pipelines, debugging something messy, or just writing Markdown — being inside Neovim feels like home. It’s not for everyone, but for me, the control, speed, and focus it gives is unmatched.\n🐍 My Language (Python)\nIt might not be the fastest or the trendiest, but Python is the tool I reach for again and again. It’s the duct tape of data — flexible, readable, and supported everywhere. Whether I’m cleaning messy CSVs, writing ETL jobs, building machine learning models, or scripting a quick fix, Python gets it done without fuss. The ecosystem is massive, the libraries are mature, and the balance between readability and power fits perfectly with how I think. It’s not just a language for me — it’s the foundation of my entire workflow.\n✨ Honorable Mentions\nThese are tools I don’t absolutely need every single day — but when I do use them, they make life noticeably better.\n📊 dbt\nIt took a while for me to “get” dbt, but once I did, it became a staple for transforming data cleanly and transparently. It’s like version control for SQL, and when paired with good documentation, it makes working with data models a joy — or at least less of a pain.\n📋 Notion\nI don’t manage my life in it, but it’s where I draft blog posts, capture ideas, and plan projects. It’s the digital equivalent of a whiteboard — clean, structured, and there when I need it.\n🧪 Jupyter Notebooks\nSometimes you just want to poke at data, test a theory, or sketch out a model. Notebooks are great for that. I don’t build anything long-term in them, but for exploratory work, they’re still hard to beat.\n📐 Texas Instruments TI-82 Stats\nThis one’s a bit of a wildcard, but it deserves a mention. The TI-82 Stats calculator was my first real exposure to the idea that math could be interactive — that I could test things, explore, and not just follow steps. I even started coding on it back in high school, making little games and cheats to get through math classes. Later, it helped me survive those chaotic calculus sessions in university. It may not be in my daily toolkit anymore, but in a way, it’s where all of this started.\n\nThat’s my current lineup — a mix of tools that help me stay productive, stay sane, and occasionally stay nostalgic. I’m always curious about what others rely on, so if you’ve got a must-have tool, a weird keyboard, or a setup hack you swear by, I’d love to hear about it. Drop a comment or hit reply — I’m here for the gear talk.\nThanks for reading,\n– N"},"Statistics/Bayesian-Statistics/bayes-theorem":{"slug":"Statistics/Bayesian-Statistics/bayes-theorem","filePath":"Statistics/Bayesian Statistics/bayes-theorem.md","title":"Bayes' Theorem","links":[],"tags":[],"content":"When you’re talking about statistics, it’s impossible not to see the direct connection to probability. And in probability, few tools are as central—or as powerful—as Bayes’ Theorem.\nBayes’ Theorem provides a principled way to update our beliefs in light of new evidence. It combines prior knowledge with new data to produce a posterior belief—making it invaluable in science, machine learning, and even everyday reasoning.\nThe theorem\nMathematically, Bayes’ Theorem is written as:\n\nP(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n\nWhere:\n\nP(A) is the prior probability of A, our initial belief.\nP(B \\mid A) is the likelihood, the probability of observing B given that A is true.\nP(B) is the evidence, the total probability of observing B under all possible hypotheses.\nP(A \\mid B) is the posterior: the updated belief about A after observing B.\n\nThis formula essentially tells us: how should we revise our belief about A after seeing B\nExample\nSuppose a certain disease affects 1% of the population. A test for the disease is 99% accurate, meaning:\n\nIf someone has the disease, the test returns positive 99% of the time.\nIf someone does not have the disease, it returns negative 99% of the time.\n\nNow, imagine you take the test and get a positive result. What is the probability you actually have the disease?\nLet:\n\nD = you have the disease\nT = test result is positive\n\nWe want to compute P(D \\mid T), the probability of disease given a positive test.\nUsing Bayes’ Theorem:\n\nP(D \\mid T) = \\frac{P(T \\mid D) \\cdot P(D)}{P(T)}\n\nWe know:\n\nP(D) = 0.01\nP(T \\mid D) = 0.99\nP(T \\mid \\neg D) = 0.01\nP(\\neg D) = 0.99\n\nThen:\n\nP(T) = P(T \\mid D) \\cdot P(D) + P(T \\mid \\neg D) \\cdot P(\\neg D) \\\n\n= 0.99 \\cdot 0.01 + 0.01 \\cdot 0.99 = 0.0198\n\nSo:\n\nP(D \\mid T) = \\frac{0.99 \\cdot 0.01}{0.0198} \\approx 0.5\n\nSurprisingly, even with a positive test, you only have about a 50% chance of actually having the disease! That’s the power of Bayes: it forces you to account for the base rate."},"Statistics/Bayesian-Statistics/bayesian-inference":{"slug":"Statistics/Bayesian-Statistics/bayesian-inference","filePath":"Statistics/Bayesian Statistics/bayesian-inference.md","title":"Bayesian Statistics","links":["Statistics/Bayesian-Statistics/bayesian-prior","Statistics/Bayesian-Statistics/bayesian-posterior","Statistics/Bayesian-Statistics/bayes-theorem"],"tags":[],"content":"The core idea behind Bayesian inference is to update our beliefs about an unknown parameter \\theta after observing data. This is done by combining a likelihood function L_n(\\theta) with a prior distribution \\pi(\\theta)—which reflects our initial beliefs about \\theta—to obtain a posterior distribution, representing our updated beliefs.\nIn many real-world situations, we have some prior knowledge about the unknown parameter \\theta. This may come from expert opinion, historical data, previous studies, or even subjective judgment.\nAfter observing new data, we use this information to update our prior belief into a posterior belief through Bayes’ theorem.\nBayes formula\nThe Bayes formula states:\n\\pi(\\theta \\mid X_1, \\ldots, X_n) \\propto \\pi(\\theta) L_n(X_1, \\ldots, X_n \\mid \\theta), \\quad \\forall \\theta \\in \\Theta\nThis means the posterior is proportional to the product of the prior and the likelihood. The constant of proportionality ensures that the posterior is a valid probability distribution (i.e., it integrates to 1).\nThe full expression includes a normalization constant in the denominator:\n\\pi(\\theta \\mid X_1, \\ldots, X_n) = \\frac{\\pi(\\theta) L_n(X_1, \\ldots, X_n \\mid \\theta)}{\\int_\\Theta \\pi(\\theta) L_n(X_1, \\ldots, X_n \\mid \\theta) d\\theta}, \\quad \\forall \\theta \\in \\Theta\nThe denominator ensures that the posterior distribution is properly scaled—it integrates to 1—but it does not generally equal 1 by itself. Instead, it plays a crucial role in turning the unnormalized posterior into a valid probability density function.\nExample: The Kiss study\nWe are interested in the proportion p of couples that turn their heads to the right when kissing.\n\nLet the data be X_1, \\dots, X_n \\overset{i.i.d}{\\sim} \\text{Ber}(p)\n\nFrequentist Approach:\n\nEstimate p using MLE\nConstruct confidence intervals for p\nPerform hypothesis testing, e.g.\nH_0: p = 0.5 vs. H_1: p \\ne 0.5\n\nBayesian Insight:\n\nBefore seeing the data, we may believe p \\approx \\frac{1}{2}\nUse the Bayesian approach to update that prior belief into a posterior using observed data\n"},"Statistics/Bayesian-Statistics/bayesian-posterior":{"slug":"Statistics/Bayesian-Statistics/bayesian-posterior","filePath":"Statistics/Bayesian Statistics/bayesian-posterior.md","title":"Posterior","links":[],"tags":[],"content":"What Is a Posterior?\nThe posterior distribution represents our updated belief about an unknown parameter \\theta after we have observed data.\nIt combines:\n\nThe prior distribution \\pi(\\theta), representing beliefs before seeing the data\nThe likelihood function L_n(\\theta) = f(X_1, \\dots, X_n \\mid \\theta), which describes how likely the observed data are under different values of \\theta\n\nInterpretation\n\nThe posterior reflects how the data have updated our prior belief\nIt gives the relative plausibility of each value of \\theta after seeing the data\nThe shape and concentration of the posterior depend on:\n\nThe data (through the likelihood)\nThe prior belief (through \\pi(\\\\theta))\n\n\n\nBayesian Updating\nBayesian inference is an iterative process:\n\nBegin with a prior\nCollect data and form a posterior\nUse this posterior as a new prior for future updates\n"},"Statistics/Bayesian-Statistics/bayesian-prior":{"slug":"Statistics/Bayesian-Statistics/bayesian-prior","filePath":"Statistics/Bayesian Statistics/bayesian-prior.md","title":"Prior","links":["Statistics/Distributions/beta","Statistics/Distributions/bernoulli","Statistics/Distributions/binomial"],"tags":[],"content":"What Is a Prior?\nThe prior expresses uncertainty about the true value of a parameter \\theta before any data is observed. It captures our beliefs or assumptions, which may be based on:\n\nExpert knowledge\nHistorical data\nSymmetry or intuition\nOr simply a desire to remain vague\n\nPriors can be:\n\nInformative, when we have strong beliefs or prior evidence.\nWeakly informative or non-informative, when we prefer to let the data drive inference.\n\nThe strength of the prior affects how much influence the data has—stronger priors can pull the posterior toward prior beliefs, especially in small-sample settings.\nExample: Kiss Study\nSuppose we believe p, the probability that a couple turns right when kissing, is close to 0.5, but we are somewhat uncertain.\nWe could encode this belief with a Beta(5, 5) prior, which is centered at 0.5 and expresses moderate confidence. This allows observed data to meaningfully update our belief while still reflecting our initial assumption.\nThe Jeffreys Prior\nWhen we lack strong prior knowledge and wish to remain as objective as possible, we can use the Jeffreys prior. It is a type of non-informative prior derived from the Fisher information and has the appealing property of being invariant under reparameterization.\nThe general form the Jeffrey prior is:\n\\pi_J(\\theta) \\propto \\sqrt{I(\\theta)} = \\sqrt{ -\\mathbb{E} \\left[ \\frac{\\partial^2}{\\partial \\theta^2} \\log f(X \\mid \\theta) \\right] }\nFor the Bernoulli/Binomial case (like the kiss study), the Jeffreys prior for the parameter p is:\n\n\\pi(p) \\propto \\frac{1}{\\sqrt{p(1 - p)}} \\quad \\Rightarrow \\quad \\text{Beta}\\left(\\tfrac{1}{2}, \\tfrac{1}{2}\\right)\n\nThis prior is more diffuse near 0 and 1, reflecting greater uncertainty in extreme values and giving slightly more weight to those possibilities than a uniform prior.\nCommon Jeffreys Priors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLikelihood / ModelParameter(s)Jeffreys PriorBernoulli/Binomialp \\in (0, 1)\\pi(p) \\propto \\dfrac{1}{\\sqrt{p(1 - p)}}Poisson\\lambda &gt; 0\\pi(\\lambda) \\propto \\dfrac{1}{\\sqrt{\\lambda}}Exponential\\lambda &gt; 0\\pi(\\lambda) \\propto \\dfrac{1}{\\lambda}Normal (known variance)\\mu \\in \\mathbb{R}\\pi(\\mu) \\propto 1Normal (known mean)\\sigma^2 &gt; 0\\pi(\\sigma^2) \\propto \\dfrac{1}{\\sigma^2}Normal (unknown mean and variance)(\\mu, \\sigma^2)\\pi(\\mu, \\sigma^2) \\propto \\dfrac{1}{\\sigma^2}Uniform [0, \\theta]\\theta &gt; x_{\\\\text{max}}\\pi(\\theta) \\propto \\dfrac{1}{\\theta}Multinomial\\vec{p} \\in \\Delta_k\\pi(\\vec{p}) \\propto \\left( \\prod_{i=1}^k p_i \\right)^{-1/2}"},"Statistics/Distributions/bernoulli":{"slug":"Statistics/Distributions/bernoulli","filePath":"Statistics/Distributions/bernoulli.md","title":"Bernoulli Distribution","links":["Statistics/Distributions/binomial","Statistics/Distributions/beta","Statistics/Bayesian-Statistics/bayesian-inference"],"tags":[],"content":"The Bernoulli distribution is one of the simplest and most fundamental probability distributions. It models a binary outcome — success or failure, yes or no, 1 or 0.\nDefinition\nA random variable X follows a Bernoulli distribution with parameter p \\in [0,1] if:\n\\mathbb{P}(X = 1) = p, \\quad \\mathbb{P}(X = 0) = 1 - p\nWe write this as:\nX \\sim \\text{Bernoulli}(p)\nProbability Mass Function (PMF)\nf(x \\mid p) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0, 1\\}\nSupport\nx \\in \\{0, 1\\}, \\quad p \\in [0, 1]\nExpectation\n\\mathbb{E}[X] = p\nVariance\n\\\\text{Var}(X) = p(1 - p)\nMaximum Likelihood Estimation (MLE)\nGiven n i.i.d. observations X_1, \\dots, X_n \\sim \\text{Bernoulli}(p), the likelihood is:\nL(p) = \\prod_{i=1}^n p^{x_i} (1 - p)^{1 - x_i}\nTaking logs:\n\\log L(p) = \\left( \\sum x_i \\right) \\log p + \\left( n - \\sum x_i \\right) \\log(1 - p)\nFirst derivative:\n\\frac{d}{dp} \\ell(p) = \\frac{S}{p} - \\frac{n - S}{1 - p}\nSecond derivative:\n\\frac{d^2}{dp^2} \\ell(p) = -\\frac{S}{p^2} - \\frac{n - S}{(1 - p)^2}\nSetting the first derivative to zero and solving yields the MLE:\n\n\\hat{p}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n X_i\n\nFisher Information\nThe Fisher information for one observation is:\nI(p) = -\\mathbb{E} \\left[ \\frac{d^2}{dp^2} \\log f(X \\mid p) \\right] = \\frac{1}{p(1 - p)}\nThis means:\n\nInformation is highest near p = 0 or p = 1\nInformation is lowest at p = 0.5, where outcomes are most unpredictable\n\nApplications\n\nCoin flips\nBinary classification\nModeling individual trials in a Binomial or logistic regression framework\n\nRelated Distributions\n\nBinomial: sum of independent Bernoulli trials\nBeta: conjugate prior for p in Bayesian analysis\n"},"Statistics/Distributions/beta":{"slug":"Statistics/Distributions/beta","filePath":"Statistics/Distributions/beta.md","title":"beta","links":[],"tags":[],"content":""},"Statistics/Distributions/binomial":{"slug":"Statistics/Distributions/binomial","filePath":"Statistics/Distributions/binomial.md","title":"binomial","links":[],"tags":[],"content":""},"Statistics/Nonparametric-Hypothesis-testing/chi-squared-goodness-of-fit-testing":{"slug":"Statistics/Nonparametric-Hypothesis-testing/chi-squared-goodness-of-fit-testing","filePath":"Statistics/Nonparametric Hypothesis testing/chi-squared-goodness-of-fit-testing.md","title":"Chi Squared Goodness of fit testing","links":[],"tags":[],"content":"What is the purpose of the test\nSuppose you have a dataset, and you want to evaluate how well it aligns with a particular discrete probability distribution. The Chi-Squared test helps you assess whether the observed frequencies in your dataset deviate significantly from what you’d expect under the hypothesized distribution.\nAssumptions\n\nThe data is binned into discrete categories (e.g. 5 bins).\nThe sample size in each bin is reasonably large (typically ≥ 5 expected counts per bin).\nObservations are independent.\n\nThe Test\nThe test statistic is computed as:\n\nT_n = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\n\nWhere:\n\n( O_i ): Observed frequency in bin ( i )\n( E_i ): Expected frequency in bin ( i ) under the null hypothesis\n( k ): Number of bins\n\nLarge values of ( T_n ) suggest a poor fit between the data and the expected distribution.\nExample\nFirst lets generate some poisson data with a \\lambda of 3.\nbin_edges = np.arange(-0.5, 10.5, 1)\nbin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n \nnp.random.seed()\npoisson_data = poisson.rvs(mu=3, size=1000)\n \nobserved, _ = np.histogram(poisson_data, bins=bin_edges)\n\nWhich we can see is definitely poisson in its distribution.\nexpected_poisson = poisson.pmf(np.arange(len(observed)), mu=3) * observed.sum()\nexpected_gaussian = (\n    norm.pdf(bin_centers, loc=3, scale=1)\n    * (bin_edges[1] - bin_edges[0])\n    * observed.sum()\n)\n \nexpected_poisson *= observed.sum() / expected_poisson.sum()\nexpected_gaussian *= observed.sum() / expected_gaussian.sum()\n \nstat_poisson, p_poisson = chisquare(f_obs=observed, f_exp=expected_poisson)\nstat_gaussian, p_gaussian = chisquare(f_obs=observed, f_exp=expected_gaussian)\nWhich in turn gives us:\n\\begin{aligned}\nT_{poiss}=1.8236827300439549\\\\\np_{poiss}=0.9939587753318262\\\\\n\\\\\nT_{N}=698370.901071352\\\\\np_{N}=0.0\n\\end{aligned}\nGiven the high p-value under the Poisson model and the extremely low p-value under the Gaussian model, the evidence strongly favours the Poisson distribution for this data.\nDegrees of freedom\ndf=\\text{number of bins}-1-\\text{number of estimated parameters}\nFor example, if you have:\n\n5 bins\nand estimate 2 parameters (like the mean and variance of a Gaussian)\n\nThen:\ndf=5-1-2=2\nGood to know\n\nThe Chi-Squared test is asymptotic, so the approximation becomes better as n \\to \\infty.\nFor small sample sizes, you should use exact p-values from tables.\n\nYou can find \\chi^2 tables here"},"Statistics/Nonparametric-Hypothesis-testing/kolmogorov-lilliefors":{"slug":"Statistics/Nonparametric-Hypothesis-testing/kolmogorov-lilliefors","filePath":"Statistics/Nonparametric Hypothesis testing/kolmogorov-lilliefors.md","title":"The Kolmogorov-Lilliefors Test","links":["Statistics/Nonparametric-Hypothesis-testing/kolmogorov-smirnov"],"tags":[],"content":"The Lilliefors test is a modification of the Kolmogorov–Smirnov (KS) test that accounts for the fact that we often don’t know the population mean and variance in advance — and instead estimate them from the sample.\nThis violates the assumptions of the regular KS test, which requires those parameters to be fixed. The Lilliefors test corrects for that by adjusting the distribution of the test statistic.\nThe null hypothesis assumes normality, but allows the parameters to be estimated from the sample. This makes the Lilliefors test more realistic than the classical KS test for normality testing.\nWhen to Use It\n\nYou’re testing for normality.\nYou estimated the mean and standard deviation from your sample.\nYour sample is continuous (not binned or discrete).\n\nThe Test\nLet ( D ) be the maximum distance between the empirical CDF of your sample and the theoretical CDF of a normal distribution fit to your data (i.e., using the sample mean and standard deviation). The Lilliefors test uses the same test statistic as KS:\nD_n = \\sup_x \\left| F_n(x) - F(x; \\hat{\\mu}, \\hat{\\sigma}) \\right|\nBut it compares it to a different null distribution, accounting for the parameter estimation.\nExample (Python)\nimport numpy as np\n \nfrom statsmodels.stats.diagnostic import lilliefors\n \n# Generate a sample from a normal distribution\nnp.random.seed(42)\nnormal_data = np.random.normal(loc=0, scale=1, size=1000)\n \n# Run the Lilliefors test (KS test adjusted for estimated mean and std)\nstat_lillie, p_lillie = lilliefors(normal_data)\n \nThis gives us:\n\\begin{aligned}\nD_{Lilliefors} &amp;= 0.0214 \\\\\np &amp;= 0.4051\n\\end{aligned}\nBecause the p-value is relatively high, we fail to reject the null hypothesis — there’s no strong evidence that the data deviates from normality.\nSummary\n\nUse the Lilliefors test instead of KS when you’re testing normality and have fit the parameters.\nThe output is a KS-like statistic and a p-value based on simulations or adjusted tables.\nIt’s a quick and useful test for normality when you can’t assume known parameters.\n"},"Statistics/Nonparametric-Hypothesis-testing/kolmogorov-smirnov":{"slug":"Statistics/Nonparametric-Hypothesis-testing/kolmogorov-smirnov","filePath":"Statistics/Nonparametric Hypothesis testing/kolmogorov-smirnov.md","title":"The Kolmogorov–Smirnov (KS) Test","links":["Statistics/Nonparametric-Hypothesis-testing/chi-squared-goodness-of-fit-testing"],"tags":[],"content":"The KS test is a non-parametric method used to determine whether a sample comes from a specific continuous distribution.\nUnlike the Chi-Squared test, which compares binned frequencies, the KS test compares the empirical cumulative distribution function (ECDF) of your sample with the theoretical cumulative distribution function (CDF) of the hypothesised distribution.\nPurpose\nTo measure the maximum vertical distance between the ECDF and the CDF of a specified distribution. Formally, the test statistic is:\nD_n = \\sup_x |F_n(x) - F(x)|\n\n( F_n(x) ): empirical CDF from the sample\n( F(x) ): theoretical CDF under the null hypothesis\n( D_n ): KS test statistic\n\nAssumptions\n\nThe hypothesised distribution is continuous\nThe sample is independent and identically distributed (i.i.d.)\nNo ties in the data (KS is sensitive to discrete data)\n\nExample: Testing Poisson Data\nTo demonstrate the KS test in its natural setting, let’s generate data from a standard normal distribution ( \\mathcal{N}(0,1) ) and compare it against:\n\nThe correct distribution: ( \\mathcal{N}(0,1) )\nA slightly incorrect distribution: ( \\mathcal{N}(1,1) )\n\nfrom scipy.stats import norm, kstest\nimport numpy as np\n \nnp.random.seed(42)\nnormal_data = norm.rvs(loc=0, scale=1, size=1000)\n \n# Test against correct distribution\nks_stat_true, p_val_true = kstest(normal_data, &#039;norm&#039;, args=(0, 1))\n \n# Test against incorrect distribution\nks_stat_wrong, p_val_wrong = kstest(normal_data, &#039;norm&#039;, args=(1, 1))\nThis yields:\n\\begin{aligned}\nD_{\\mathcal{N}(0,1)} &amp;= 0.0173 \\\\\np_{\\mathcal{N}(0,1)} &amp;= 0.9197 \\\\\n\\\\\nD_{\\mathcal{N}(1,1)} &amp;= 0.3916 \\\\\np_{\\mathcal{N}(1,1)} &amp;= 1.11 \\times 10^{-138}\\\\\n\\\\\n\\end{aligned}\nInterpretation\n\nThe high p-value when testing against (\\mathcal{N}(0,1) ) suggests the sample is consistent with the hypothesised distribution.\nThe extremely low p-value when testing against ( \\mathcal{N}(1,1) ) strongly rejects the fit.\n"},"Statistics/Nonparametric-Hypothesis-testing/qq-plots":{"slug":"Statistics/Nonparametric-Hypothesis-testing/qq-plots","filePath":"Statistics/Nonparametric Hypothesis testing/qq-plots.md","title":"QQ-Plots","links":[],"tags":[],"content":"A QQ-plot (Quantile–Quantile plot) is a graphical tool used to assess whether your sample data comes from a specific theoretical distribution — most commonly, the normal distribution.\nIt works by plotting:\n\nThe quantiles of the theoretical distribution (x-axis)\nAgainst the quantiles of your sample data (y-axis)\n\nMore specifically, it plots the points:\n\n\\left( F^{-1}\\left(\\frac{1}{n}\\right), F_n^{-1}\\left(\\frac{1}{n}\\right) \\right), \n\n\\left( F^{-1}\\left(\\frac{2}{n}\\right), F_n^{-1}\\left(\\frac{2}{n}\\right) \\right), \\ldots,\n\n\\left( F^{-1}\\left(\\frac{n-1}{n}\\right), F_n^{-1}\\left(\\frac{n-1}{n}\\right) \\right)\n\nIf your data closely follows the target distribution, the points will fall approximately along the 45-degree line (y = x).\nInterpreting the QQ-plot\nBelow are QQ-plots illustrating common deviations from normality:\n\n\nRight Skew (Exp(1)): Long right tail; points bend upward to the right.\nLeft Skew (-Exp(1)): Long left tail; points bend downward to the left.\nHeavy Tails (t-distribution, df=3): More extreme values than normal; points curve outwards at both ends.\nLight Tails (Uniform[0,1]): Fewer extreme values; QQ-plot shows an S-shape hugging the diagonal in the middle.\n"},"Stupid-data-science/ufo":{"slug":"Stupid-data-science/ufo","filePath":"Stupid data science/ufo.md","title":"Here Are the Most UFO-Friendly States in the U.S.","links":[],"tags":[],"content":"\nWhere in America are you most likely to spot something strange in the sky — and actually report it?\n\nEach year, thousands of Americans file reports of unidentified flying objects. But not all states are created equal when it comes to scanning the skies. Some places seem to be absolute UFO magnets — and it’s not always where you’d expect.\nSo we asked ourselves:\nWhat makes a state “UFO-friendly”?\nWe landed on two ingredients:\n\n🛸 How often people report UFO sightings (scaled per million residents)\n🌾 How rural the state is, which we used as a rough stand-in for things like open skies, less light pollution, and maybe — just maybe — a stronger belief in the unexplained.\n\nTogether, these make up our UFO Visibility Index — a totally unscientific but super fun score to rank every U.S. state’s UFO-spotting potential.\nLet’s dig in.\n\n👀 UFO Sightings per Million People\n\nThis map shows how many UFO sightings each state has reported per million residents — giving small but active states like Vermont a fair shot against behemoths like California.\n🧠 Top Performers\n\nVermont, Montana, and Washington top the list with the highest sightings-per-million.\nThe Pacific Northwest and Northeast seem especially active — is it all the forests? The coffee? Or just more sky-gazing?\n\nWhatever it is, these folks are paying attention.\n\n🌾 Rural Percentage of Population\n\nWhy does rurality matter? Well, it could mean clearer skies, fewer city lights, and maybe even a greater willingness to believe or report something strange up there.\nThis map shows what percent of each state’s population lives in rural areas.\n🏞️ Leaders in Rural Life\n\nVermont, Maine, and West Virginia take the rural crown.\nThe most urbanized? California, Nevada, and New Jersey — all clocking in under 10% rural population.\n\nNot saying rural folks believe more in aliens, but… they do have better stargazing conditions. 👀\n\n🚀 The UFO Visibility Index\n\nNow comes the fun part.\nWe combined the two normalized metrics — sightings-per-million and rural population percentage — into a single UFO Visibility Index using Min-Max scaling.\nSo what does this index show?\n\nIt highlights states where people are reporting sightings and live in areas where sky visibility is likely better.\nHigher index = better “UFO spotting conditions” (in theory, of course 😎).\n\n🥇 Top UFO-Friendly States:\n\nMaine, Alaska, and Montana dominate the rankings.\nThese states strike a curious balance: lots of rural land and people eager to report what’s flying through it.\n\n\n🧪 Method Notes\nThis was all done in a Python notebook — no fancy modeling, just some data wrangling, normalization, and choropleth mapping for fun.\nIf you’re curious, the correlation between rural population percentage and sightings-per-million sits at around 0.25. Not huge, but definitely not nothing.\nCheck out the full notebook here to explore the code and data sources.\n\n👋 Final Thoughts\nSo there you have it — if you’re hoping to catch something weird in the sky, Vermont, Maine, Alaska, and Montana might be your best bets. Whether it’s clearer skies, fewer city lights, or just a greater sense of curiosity, some states definitely seem more tuned in to what’s up there.\nWant to poke around the data yourself or see how this was built?\nCheck out the full notebook and code here.\nKeep your eyes on the skies 👽\n\n📚 Sources\n1. Kaggle (n.d.)\nUFO Sightings. Available at: www.kaggle.com/datasets/NUFORC/ufo-sightings (Accessed: 22 June 2025).\n2. United States Census Bureau (2024)\nState Population Totals and Components of Change: 2020–2024. Available at: www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html (Accessed: 22 June 2025).\n3. Federal Housing Finance Agency (FHFA) (2023)\nWho Lives in Rural America? Available at: www.fhfa.gov/blog/insights/who-lives-in-rural-america (Accessed: 22 June 2025).\n4. United Health Foundation (2025)\nExplore Rural Population in the United States. America’s Health Rankings. Available at: www.americashealthrankings.org/explore/measures/rural_population (Accessed: 22 June 2025)."},"index":{"slug":"index","filePath":"index.md","title":"Welcome to My Notes","links":[],"tags":[],"content":"This is a public digital garden built with Quartz.\nI’ve always been the kind of person who jots down notes on everything I read or think about—whether it’s quick scribbles on my phone or spontaneous voice memos.\nTechnology and science have always fascinated me. That curiosity led me to study systems science and eventually work at the intersection of math, data, and statistics.\nHere, you’ll find a bit of everything—from thoughts and reflections to the occasional ridiculous analysis of a dataset no one asked for.\nThis site is a work in progress (and probably always will be). Feel free to explore as it grows.\nBest,\nNathan"}}